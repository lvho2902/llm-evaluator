{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c926d26",
   "metadata": {},
   "source": [
    "### Test Vicuna\n",
    "\n",
    "* `Background`: https://python.langchain.com/en/latest/modules/models/llms/integrations/llamacpp.html\n",
    "* Reproduce the logic that happens in API of the `auto-evaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.llms import Replicate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4e440",
   "metadata": {},
   "source": [
    "`Load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(files):\n",
    "\n",
    "    # Load docs\n",
    "    # IN: List of upload files (from Streamlit)\n",
    "    # OUT: str\n",
    "    # TODO: Support multple docs, Use Langchain loader\n",
    "\n",
    "    all_text = \"\"\n",
    "    for file_path in files:\n",
    "        file_extension = os.path.splitext(file_path)[1]\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_reader = pypdf.PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            all_text += text\n",
    "        elif file_extension == \".txt\":\n",
    "            loader = UnstructuredFileLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text += docs[0].page_content\n",
    "        else:\n",
    "            print('Please provide txt or pdf.')\n",
    "\n",
    "    return all_text\n",
    "\n",
    "fis = glob.glob(\"docs/karpathy-lex-pod/*txt\")\n",
    "text = load_docs(fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e75a9c",
   "metadata": {},
   "source": [
    "`Split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3370cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(text, chunk_size, overlap, split_method):\n",
    "\n",
    "    # Split text\n",
    "    # IN: text, chunk size, overlap\n",
    "    # OUT: list of str splits\n",
    "    # TODO: Add parameter for splitter type\n",
    "\n",
    "    print(\"`Splitting doc ...`\")\n",
    "    if split_method == \"RecursiveTextSplitter\":\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                       chunk_overlap=overlap)\n",
    "    elif split_method == \"CharacterTextSplitter\":\n",
    "        text_splitter = CharacterTextSplitter(separator=\" \",\n",
    "                                              chunk_size=chunk_size,\n",
    "                                              chunk_overlap=overlap)\n",
    "    splits = text_splitter.split_text(text)\n",
    "    return splits\n",
    "\n",
    "split_method = \"RecursiveTextSplitter\" \n",
    "overlap = 20\n",
    "chunk_size = 500\n",
    "splits = split_texts(text, chunk_size, overlap, split_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c35fd",
   "metadata": {},
   "source": [
    "`Test model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ab75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** update with your local path *** ###\n",
    "LLAMA_CPP_PATH = \"/Users/31treehaus/Desktop/AI/llama.cpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264c05d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pass the raw question into the prompt template.\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    \n",
    "    model_path=LLAMA_CPP_PATH+\"models/vicuna_13B/ggml-vicuna-13b-4bit.bin\",\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    n_threads=6,\n",
    "    n_ctx=2048,\n",
    "    use_mlock=True)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt,llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a09e14",
   "metadata": {},
   "source": [
    "`Make Retrieval Chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d174af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(splits, retriever_type, embeddings, num_neighbors):\n",
    "\n",
    "    # Make document retriever\n",
    "    # IN: list of str splits, retriever type, embedding type, number of neighbors for retrieval\n",
    "    # OUT: retriever\n",
    "\n",
    "    print(\"`Making retriever ...`\")\n",
    "    # Set embeddings\n",
    "    if embeddings == \"OpenAI\":\n",
    "        embd = OpenAIEmbeddings()\n",
    "    elif embeddings == \"HuggingFace\":\n",
    "        embd = HuggingFaceEmbeddings()\n",
    "\n",
    "    # Select retriever\n",
    "    if retriever_type == \"similarity-search\":\n",
    "        try:\n",
    "            vectorstore = FAISS.from_texts(splits, embd)\n",
    "        except ValueError:\n",
    "            print(\"`Error using OpenAI embeddings (disallowed TikToken token in the text). Using HuggingFace.`\")\n",
    "            vectorstore = FAISS.from_texts(splits, HuggingFaceEmbeddings())\n",
    "        retriever = vectorstore.as_retriever(k=num_neighbors)\n",
    "    elif retriever_type == \"SVM\":\n",
    "        retriever = SVMRetriever.from_texts(splits,embd)\n",
    "    elif retriever_type == \"TF-IDF\":\n",
    "        retriever = TFIDFRetriever.from_texts(splits)\n",
    "    return retriever\n",
    "\n",
    "retriever_type = \"similarity-search\"\n",
    "embeddings = \"OpenAI\"\n",
    "num_neighbors = 3\n",
    "retriever = make_retriever(splits, retriever_type, embeddings, num_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205c92d",
   "metadata": {},
   "source": [
    "`Make Prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bed6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. Use three sentences maximum. \n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: Think step by step \"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_llm(model):\n",
    "    \"\"\"\n",
    "    Make LLM\n",
    "    @param model: LLM to use\n",
    "    @return: LLM\n",
    "    \"\"\"\n",
    "\n",
    "    if model in (\"gpt-3.5-turbo\", \"gpt-4\"):\n",
    "        llm = ChatOpenAI(model_name=model, temperature=0)\n",
    "    elif model == \"anthropic\":\n",
    "        llm = ChatAnthropic(temperature=0)\n",
    "    elif model in (\"vicuna-7b\",\"vicuna-13b\"):\n",
    "        callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        if model == \"vicuna-7b\":\n",
    "            llm = LlamaCpp(\n",
    "                model_path=LLAMA_CPP_PATH+\"models/vicuna_7B/ggml-vicuna-7b-q4_0.bin\",\n",
    "                callback_manager=callback_manager,\n",
    "                verbose=True,\n",
    "                n_threads=6,\n",
    "                n_ctx=2048,\n",
    "                use_mlock=True)\n",
    "        else:\n",
    "            llm = LlamaCpp(\n",
    "                model_path=LLAMA_CPP_PATH+\"models/vicuna_13B/ggml-vicuna-13b-4bit.bin\",\n",
    "                callback_manager=callback_manager,\n",
    "                verbose=True,\n",
    "                n_threads=6,\n",
    "                n_ctx=2048,\n",
    "                use_mlock=True)\n",
    "    return llm\n",
    "\n",
    "llm = make_llm('vicuna-13b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48abfe",
   "metadata": {},
   "source": [
    "`Eval Set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "test_dataset = pd.read_csv(\"docs/karpathy-lex-pod/karpathy-pod-eval.csv\")\n",
    "qus = []\n",
    "for i in test_dataset.index:\n",
    "    question = test_dataset.loc[i, \"question\"]\n",
    "    answer = test_dataset.loc[i, \"answer\"]\n",
    "    data = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    qus.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e60bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003593a",
   "metadata": {},
   "source": [
    "`Run Inference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f675d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain(llm, retriever, retriever_type):\n",
    "    \"\"\"\n",
    "    Make retrieval chain\n",
    "    @param llm: model\n",
    "    @param retriever: retriever\n",
    "    @param retriever_type: retriever type\n",
    "    @return: QA chain or Llama-Index retriever, which enables QA\n",
    "    \"\"\"\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT}\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           chain_type=\"stuff\",\n",
    "                                           retriever=retriever,\n",
    "                                           chain_type_kwargs=chain_type_kwargs,\n",
    "                                           input_key=\"question\")\n",
    "    return qa_chain\n",
    "\n",
    "qa_chain = make_chain(llm, retriever, retriever_type)\n",
    "result = qa_chain(qus[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469143b0",
   "metadata": {},
   "source": [
    "`Test Vicuna endpoint on Replicate`\n",
    "\n",
    "Deployed to `A100` on Replicate.\n",
    "\n",
    "* `max_length` maximum length of the prompt + the output for a given generation\n",
    "* `context window` 2048 tokens\n",
    "\n",
    "Useful reference:\n",
    "https://github.com/replicate/cog-vicuna-13b/issues/3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a38076",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Replicate(model=\"replicate/vicuna-13b:e6d469c2b11008bb0e446c3e9629232f9674581224536851272c54871f84076e\",\n",
    "        input={\"temperature\": 0.75, \"max_length\": 3000, \"top_p\":0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81af87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_utils import QA_CHAIN_PROMPT, QA_CHAIN_PROMPT_LLAMA\n",
    "chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT_LLAMA}\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type_kwargs=chain_type_kwargs,\n",
    "                                       input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81694537",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain(qus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ada58",
   "metadata": {},
   "source": [
    "`Test Mosaic`\n",
    "\n",
    "LangChain docs: \n",
    "\n",
    "https://python.langchain.com/en/latest/modules/models/text_embedding/examples/mosaicml.html\n",
    "\n",
    "Args: \n",
    "\n",
    "https://docs.mosaicml.com/en/latest/inference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9371e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import MosaicML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f9fea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "llm = MosaicML(inject_instruction_format=True,model_kwargs={'do_sample': False,'max_length': 3000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. Use three sentences maximum. \n",
    "{context}\n",
    "Question: {question} \"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07232f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT}\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type_kwargs=chain_type_kwargs,\n",
    "                                       input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain(qus[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
